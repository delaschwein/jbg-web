~~ Bibtex | inproceedings
~~ Title |  Evaluation Examples Are Not Equally Informative: How Should That Change NLP Leaderboards?
~~ Author | Pedro Rodriguez and Joe Barrow and Alexander Hoyle and John P. Lalor and Robin Jia
~~ Booktitle | Association of Computational Linguistics
~~ Year | 2021
~~ Category | Empirical Human Data Collection
~~ Category | Variational Inference
~~ Category | Question Answering
~~ Venue | Refereed Conference
~~ Project | CAREER*../projects/IIS-1652666.html
~~ Url | docs/2021_acl_leaderboard.pdf
~~ Link | Results and Code*https://leaderboard.pedro.ai/
~~ Link | ACL Presentation*https://www.youtube.com/watch?v=akUxtt21Mlc
~~ Paper Read Aloud (video) | https://youtu.be/BYzFuYzEqI8
~~ Public | Modern machine learning research often depends on leaderboards to monitor progress, but beyond rankings of approaches, this does not help us better understand our problems or our systems very well.  This paper introduces probabilistic models inspired by educational testing (think year-end tests in school) to better understand how computers can answer questions and whether we are asking the right questions.
~~ Embed | <iframe width="300" height="160" src="https://www.youtube.com/embed/akUxtt21Mlc" frameborder="0" allowfullscreen></iframe>
