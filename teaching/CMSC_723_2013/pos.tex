\documentclass[11pt,twoside]{article}
\usepackage{alltt}
\usepackage{comment}

\textwidth=6.5in
\oddsidemargin=0.25in
\evensidemargin=0.25in
\topmargin=-0.1in
\footskip=0.8in
\parindent=0.0cm
\parskip=0.3cm
\textheight=8.00in
\setcounter{tocdepth} {3}
\setcounter{secnumdepth} {2}
\sloppy

\newcounter{lecnum}

\newcommand{\answer}[1]{{\bf #1}}

\newcommand{\lecture}[5]{
   \pagestyle{myheadings} \thispagestyle{plain} \newpage
\setcounter{lecnum}{#1} \setcounter{page}{1} \noindent
\begin{center}
\framebox{\vbox{
{Homework #1 \hfill   #2} \\
\vspace{2mm}
{\bf Part of Speech Tagging}\\
\vspace{2mm}
{Due: #3   \hfill #4}\\
\vspace{4mm}}}
\end{center}

\markboth{Homework #1: #3}{Homework #1: #3}\vspace*{4mm}}

\begin{document}

% Add your name in the last bracket if you use this template
\lecture{4}{CMSC 723}{October 14, 2013}{}

\section{Tagging and Tag Sets (20 points)}

\subsection{When taggers go bad (10 points)}

Consider the following sentences:
\begin{enumerate}
\item British Left Waffles on Falkland Islands
\item Teacher Strikes Idle Kids
\item Clinton Wins Budget; More Lies Ahead
\item Juvenile Court to Try Shooting Defendant
\end{enumerate}

(You're also more than welcome to create or find another sentence that
is similarly confusing.)  Choose one of these sentences and tag it in
two different (but plausible) ways.  


\subsection{Exploring the tag set (10 points)}
There are 265 distinct words in the Brown Corpus having exactly four possible tags (assuming nothing is done to normalize the word forms).
\begin{enumerate}
\item Create a table with the integers 1\dots 10 in one column, and the number of distinct
words in the corpus having $\{1, \dots, 10\}$ distinct tags.
\item For the word with the greatest number of distinct tags, print out sentences from
the corpus containing the word, one for each possible tag.

\end{enumerate}

\section{Viterbi Algorithm (80 Points)}

Consider the following sentences written in Klingon.  For each
sentence, the part of speech of each ``word'' has been given (for ease
of translation, some prefixes/suffixes have been treated as words),
along with a translation.  Using these training sentences, we're going
to build a hidden Markov model to predict the part of speech of an
unknown sentence using the Viterbi algorithm.

\begin{tabular}{llllll}
  N  & PRO  & V   & N         & PRO  \\
pa'Daq   & ghah & taH & tera'ngan & 'e   \\
room (inside)    & he   & is  & human     & of   \\ 
\multicolumn{6}{c}{{\em The human is in the room}} \\
\end{tabular}

\begin{tabular}{llll}
V   & N      & V    & N              \\
ja'chuqmeH & rojHom & neH  & tera'ngan        \\
in order to parley  & truce  & want & human \\
\multicolumn{4}{c}{{\em The enemy commander wants a truce in order to parley}}
\end{tabular}

\begin{tabular}{lllllll}
N         & V      & N     & CONJ & N         & V    & N \\
tera'ngan & qIp   & puq   & 'eg  & puq & qIp  & tera'ngan \\
human     & bit  & child & and  & child & bit & child \\
\multicolumn{7}{c}{{\em The child bit the human, and the human bit the child}}
\end{tabular}

\subsection{Emission Probability (25 points)}

Compute the frequencies of each part of speech in the table below for
nouns and verbs.  We'll use a smoothing factor of 0.1 (as discussed in
class) to make sure that no event is impossible; add this number to
all of your observations.  Two parts of speech have already been
done for you.  After you've done this, compute the emission probabilities in a similar table.

\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
   & NOUN & VERB & CONJ & PRO \\
\hline
'e &   &   &  0.1    &  1.1   \\
\hline
'eg &     &     &  1.1    &  0.1 \\
\hline
ghaH &     &    &  0.1    &  1.1  \\
\hline
ja'chuqmeH &     &     &  0.1    &  0.1   \\
\hline
legh &     &     &  0.1    &  0.1 \\
\hline
neH &     &     &  0.1    &  0.1   \\
\hline
pa'Daq &     &     &  0.1    &  0.1 \\
\hline
puq &     &     &  0.1    &  0.1 \\
\hline
qIp &     &     &  0.1    &  0.1 \\
\hline
rojHom &     &     &  0.1    &  0.1\\
\hline
taH &     &     &  0.1    &  0.1 \\
\hline
tera'ngan &     &     &  0.1    &  0.1 \\
\hline
yaS &     &     &  0.1    &  0.1   \\
\hline
\end{tabular}
\end{center}

\subsection{Start and Transition Probability (25 points)}

Now, for each part of speech, total the number of times it
transitioned to each other part of speech.  Again, use a smoothing
factor of 0.1.  After you've done this, compute the start and transition probabilities.

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
      & NOUN & VERB & CONJ & PRO \\
\hline
START &  &   &  &   \\
\hline
N     &   &   & 1.1  & 2.1  \\
\hline
V     &   &   & 0.1  & 0.1  \\
\hline
CONJ  &   &   &  0.1 & 0.1 \\
\hline
PRO   &  &   &  0.1 & 0.1 \\
\hline
\end{tabular}
\end{center}

\subsection{Viterbi Decoding (30 points)}

Now consider the following sentence: ``tera'ngan legh yaS''.  

\begin{enumerate}
\item Suppose that we knew ``legh'' were a pronoun (it's not, but pronouns often act like its true part of speech in Klingon).  What would be the probability of each of the four parts of speech for ``yaS''?

\item Create the decoding matrix of this sentence for nouns and verbs (ignore other parts of speech if you want; doing so won't prevent you from finding the right answer).  You should have at least six numbers: $\log \delta_{n}(k)$ for $n=1\dots3$ and $k$ for both nouns and verbs.

\item What is the most likely sequence of parts of speech?

\item What is the probability of your previous answer?

\item (For fun, not for credit) What do you think this sentence means?  What word is the subject of the sentence?

\end{enumerate}



\end{document}