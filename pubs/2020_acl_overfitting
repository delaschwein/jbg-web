~~ Bibtex | inproceedings
~~ Authors | Mozhi Zhang and Yoshinari Fujinuma and Michael J. Paul and Jordan Boyd-Graber
~~ Title |   Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries
~~ Booktitle | Association for Computational Linguistics
~~ Year | 2020
~~ Location | The Cyberverse Simulacrum of Seattle
~~ Category | Deep Learning
~~ Url | docs/2020_acl_refine.pdf
~~ Link | Preprint*https://arxiv.org/abs/2005.00524
~~ Category | Multilingual Corpora
~~ Project | BETTER*../projects/better.html
~~ Venue | Refereed Conference
~~ Acceptance | 17.6
~~ Link | Video*http://youtu.be/yVN47wGkCko
~~ Embed | <iframe width="300" height="160" src="https://www.youtube.com/embed/yVN47wGkCko" frameborder="0" allowfullscreen></iframe>
~~ Public | Computers need to represent words in a computer-readable way.Â  This work talks about how slightly moving these representations to get closer to a small list of translations (like from a dictionary) after doing fancy machine learning works better on downstream tasks (e.g., guessing grammatical category of a word) but hurts on asking the algorithm for translations of unseen words.