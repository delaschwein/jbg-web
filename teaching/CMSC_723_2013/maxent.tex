\documentclass[11pt,twoside]{article}
\usepackage{alltt}
\usepackage{comment}
\usepackage{mathtools}


\textwidth=6.5in
\oddsidemargin=0.25in
\evensidemargin=0.25in
\topmargin=-0.1in
\footskip=0.8in
\parindent=0.0cm
\parskip=0.3cm
\textheight=8.00in
\setcounter{tocdepth} {3}
\setcounter{secnumdepth} {2}
\sloppy

\newcounter{lecnum}

\newcommand{\answer}[1]{{\bf #1}}

\newcommand{\lecture}[5]{
   \pagestyle{myheadings} \thispagestyle{plain} \newpage
\setcounter{lecnum}{#1} \setcounter{page}{1} \noindent
\begin{center}
\framebox{\vbox{
{Homework #1 \hfill   #2} \\
\vspace{2mm}
{\bf Maximum Entropy Models}\\
\vspace{2mm}
{Due: #3   \hfill #4}\\
\vspace{4mm}}}
\end{center}

\markboth{Homework #1: #3}{Homework #1: #3}\vspace*{4mm}}

\begin{document}

% Add your name in the last bracket if you use this template
\lecture{7}{CMSC 723}{November 18\textsuperscript{th}, 2013}{}

\section{MaxEnt Math (40 pts)}
% \large{\textbf{1.}}\\
Suppose we have an unregularized maximum entropy model with input $x$ and output $y$. The output is an $n$-dimensional binary vector of the form $y$ = $<y_1, y_2, \ldots, y_n>$, where $y_i \in \{0, 1\}$. Our model has the following $n$ features:

\begin{align*}
& f_1(x, y) = 
	\begin{cases}
		1, & \text{if } y_1 = 1\\
		0, & \text{otherwise}
	\end{cases} \\
& f_2(x, y) = 
	\begin{cases}
		1, & \text{if } y_2 = 1\\
		0, & \text{otherwise}
	\end{cases} \\
& \vdots \\
& f_n(x, y) = 
	\begin{cases}
		1, & \text{if } y_n = 1\\
		0, & \text{otherwise}
	\end{cases} 
\end{align*}

The probability distribution for this model is given below, where $\lambda$ is a vector of feature weights and the denominator normalizes over all possible $n$-dimensional binary vectors:

\begin{equation}\label{eq:1}
P \left( y \hspace{1pt}\vert\hspace{1pt} x \right) = \frac{e^{\lambda_1 f_1(x,y) + \lambda_2 f_2(x,y) + \dots + \lambda_n f_n(x,y)}}
{\displaystyle\sum\limits_{y'} e^{\lambda_1 f_1(x,y') + \lambda_2 f_2(x,y') + \dots + \lambda_n f_n(x,y')}}
\end{equation}

Rewrite the right-hand side of \eqref{eq:1} to show that 

\[
P \left( y \hspace{1pt}\vert\hspace{1pt} x \right) = \prod\limits_{i=1}^n P_i \left( y_i \hspace{1pt}\vert\hspace{1pt} x \right)
\]

where each $P_{i}$ is the probability distribution specified by a maximum entropy model with a single feature.

\textbf{Hint:} If we define the single feature $f_i'$ for $P_i$ as below, then $f_i' = f_i(x, y)$.
\begin{align*}
& f_i'(x, y_i) = 
	\begin{cases}
		1, & \text{if } y_i = 1\\
		0, & \text{otherwise}
	\end{cases} \\
\end{align*}


\section{Word Root Identification (60 pts)}
We're going to design a maximum entropy model to identify the root of a given word. The root may be either a prefix of the given word (\emph{acrobat} comes from the Greek root \emph{acro}, which has meanings such as \emph{height} and \emph{top}), or a suffix (the root of \emph{inspect} is \emph{spect}, which means \emph{to look}). For this problem, we'll ignore cases in which the root occurs in the middle of the word (e.g. \emph{vert} in \emph{advertisement}).

The probability of root $r$ given word $w$ is as follows:

\[
P \left( r \hspace{1pt}\vert\hspace{1pt} w \right) = \frac{e^{\lambda f(w,r)}}
{\displaystyle\sum\limits_{r'} e^{\lambda f(w,r')}}
\]

where the denominator normalizes over all possible prefixes and suffixes of $w$. As an example, for the word \emph{lemon}, we have to consider the set of prefixes (\emph{lemo}, \emph{lem}, \emph{le}, \emph{l}), the set of suffixes (\emph{emon}, \emph{mon}, \emph{on}, \emph{n}), and \emph{lemon} itself.

We'd like this distribution to give us the following probabilities:
\begin{align*}
& P \left( anti \hspace{1pt}\vert\hspace{1pt} antipathy \right) = 0.9 \\
& P \left( hyper \hspace{1pt}\vert\hspace{1pt} hypersonic \right) = 0.7 \\
& P \left( homeo \hspace{1pt}\vert\hspace{1pt} homeopathy \right) = 0.8 \\
& P \left( sect \hspace{1pt}\vert\hspace{1pt} intersect \right) = 0.5 \\
& P \left( super \hspace{1pt}\vert\hspace{1pt} supersonic \right) = 0.7 \\
& P \left( sect \hspace{1pt}\vert\hspace{1pt} sector \right) = 0.6 \\ 
& P \left( insect \hspace{1pt}\vert\hspace{1pt} insect \right) = 0.2
\end{align*}

\subsection{Feature Design (40 pts)}
Your task is to design a set of indicator features (binary features whose only possible values are 0 and 1) that can represent this distribution. While many possible solutions exist, you'll be penalized if you use more than four features. Below is an example feature:

\begin{align*}
& f_1(w, r) = 
	\begin{cases}
		1, & \text{if } w = ``example"\\
		0, & \text{otherwise}
	\end{cases} \\
\end{align*}

In devising your features, you should pay attention to the number of {\bf distinct} probabilities, not the exact probabilities.


\subsection{Using Your Features to Predict Roots (20 pts)}
Let's say you're given the parameter vector $\lambda$, where $\lambda_1, \lambda_2, \dots, \lambda_n$ are weights for each of your $n$ features. Write expressions for the following probabilities:
\begin{align*}
& P \left( a \hspace{1pt}\vert\hspace{1pt} apathy \right)\\
& P \left( sect \hspace{1pt}\vert\hspace{1pt} bisect \right)\\
& P \left( mason \hspace{1pt}\vert\hspace{1pt} masonic \right)\\
& P \left( plutocracy \hspace{1pt}\vert\hspace{1pt} plutocracy \right)\\
\end{align*}


% \subsection{Solving for $\lambda$ (20 pts)}
% What values for $\lambda_1, \lambda_2, \dots, \lambda_n$ yield the distribution specified in the problem description?

\end{document}